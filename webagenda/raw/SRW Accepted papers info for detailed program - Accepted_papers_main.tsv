Number	Track	Subtrack	Title	Authors	Abstract
SRW_1	Student Research Workshop	NLP Applications	Heterogeneous-Graph Reasoning and Fine-Grained Aggregation for Fact Checking	Hongbin Lin	Fact checking is a challenging task that requires corresponding evidences to verify the property of a claim based on reasoning. Previous studies generally i) construct the graph by treating each evidence-claim pair as node which is a simple way that ignores to exploit their implicit interaction, or building a fully-connected graph among claim and evidences where the entailment relationship between claim and evidence would be considered equally to the semantic relationship among evidences; ii) aggregate evidences equally without considering their different stances towards the verification of fact. Towards the above issues, we propose a novel heterogeneous-graph reasoning and fine-grained aggregation model, with two following modules: 1) a heterogeneous graph attention network module to distinguish different types of relationships within the constructed graph; 2) fine-grained aggregation module which learns the implicit stance of evidences towards the prediction result in details. Extensive experiments on the benchmark dataset demonstrate that our proposed model achieves much better performance than state-of-the-art methods.
SRW_13	Student Research Workshop	Machine Translation and Multilinguality	Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention	Ayush K Chakravarthy, Jacob Labe Russin, Randall O'Reilly	Systematicity is thought to be a key inductive bias possessed by humans that is lacking in standard natural language processing systems such as those utilizing transformers. In this work, we investigate the extent to which the failure of transformers on systematic generalization tests can be attributed to a lack of linguistic abstraction in its attention mechanism. We develop a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention distributions (i.e., queries and keys) at each layer, and a filler stream determines the values. Our results show that when abstract role labels are assigned to input sequences and provided to the role stream, systematic generalization is improved. 
SRW_15	Student Research Workshop	Dialogue and Interactive Systems	Grounding in social media: An approach to building a chit-chat dialogue model	Ritvik Choudhary, Daisuke Kawahara	Building open-domain dialogue systems capable of rich human-like conversational ability is one of the fundamental challenges in language generation. However, even with recent advancements in the field, existing open-domain generative models fail to capture and utilize external knowledge, leading to repetitive or generic responses to unseen utterances. Current work on knowledge-grounded dialogue generation primarily focuses on persona incorporation or searching a fact-based structured knowledge source such as Wikipedia. Our method takes a broader and simpler approach, which aims to improve the raw conversation ability of the system by mimicking the human response behavior through casual interactions found on social media. Utilizing a joint retriever-generator setup, the model queries a large set of filtered comment data from Reddit to act as additional context for the seq2seq generator. Automatic and human evaluations on open-domain dialogue datasets demonstrate the effectiveness of our approach.
SRW_16	Student Research Workshop	Speech and Multimodality	Towards Unsupervised Speech Synthesis	Alexander H. Liu, Cheng-I Lai, James R. Glass	In this paper, we introduce the first unsupervised speech synthesis system that can be built with a simple recipe. The framework is based on a recently developed unsupervised speech recognition system and an existing neural-based speech synthesis paradigm. With unpaired audio, unpaired text, and lexicon, our method enables speech synthesis without the need for human-labeled corpus. Our preliminary result shows the unsupervised model achieved similar performance to its supervised counterpart in human opinion score.
SRW_17	Student Research Workshop	Summarization	ExtraPhrase: Efficient Data Augmentation for Abstractive Summarization	Mengsay Loem, Sho Takase, Masahiro Kaneko, Naoaki Okazaki	Neural models trained with large amount of parallel data have achieved impressive performance in abstractive summarization tasks. However, large-scale parallel corpora are expensive and challenging to construct. In this work, we introduce a low-cost and effective strategy, ExtraPhrase, to augment training data for abstractive summarization tasks.  ExtraPhrase constructs pseudo training data in two steps: extractive summarization and paraphrasing. We extract major parts of an input text in the extractive summarization step and obtain its diverse expressions with the paraphrasing step. Through experiments, we show that ExtraPhrase improves the performance of abstractive summarization tasks by more than 0.50 points in ROUGE scores compared to the setting without data augmentation. ExtraPhrase also outperforms existing methods such as back-translation and self-training. We also show that ExtraPhrase is significantly effective when the amount of genuine training data is remarkably small, i.e., a low-resource setting. Moreover, ExtraPhrase is more cost-efficient than the existing approaches
SRW_19	Student Research Workshop	Machine Learning for NLP	Regularized Training of Nearest Neighbor Language Models	Jean-Francois Ton, Walter Talbott, Shuangfei Zhai, Joshua M. Susskind	Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon NN-LM \citep{khandelwal20generalization}, which uses a pre-trained language model together with an exhaustive NN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the NN-LM performance by instead training a LM with the knowledge that we will be using a NN post-hoc. We achieved significant improvement using our method on language modeling tasks on \texttt{WIKI-2} and \texttt{WIKI-103}. The main phenomenon that we encounter is that adding a simple L2 regularization on the activations (not weights) of the model, a transformer, improves the post-hoc NN classification performance. We explore some possible reasons for this improvement.  In particular, we find that the added L2 regularization seems to improve the performance for high-frequency words without deteriorating the performance for low frequency ones.
SRW_22	Student Research Workshop	Computational Social Science and Cultural Analytics	"Again, Dozens of Refugees Drowned": A Computational Study of Political Framing Evoked by Presuppositions	Qi Yu	Earlier NLP studies on framing in political discourse have focused heavily on shallow classification of issue framing, while framing effect arising from pragmatic cues remains neglected. We put forward this latter type of framing as "pragmatic framing". To bridge this gap, we take presupposition-triggering adverbs such as 'again' as a study case, and quantitatively investigate how different German newspapers use them to covertly evoke different attitudinal subtexts in their report on the event "European Refugee Crisis" (2014-2018). Our study demonstrates the crucial role of presuppositions in framing, and emphasizes the necessity of more attention on pragmatic framing in the research of automated framing detection.
SRW_23	Student Research Workshop	Language Generation	Methods for Estimating and Improving Robustness of Language Models	Michal Stefanik	Despite their outstanding performance, large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem. This proposal investigates a common denominator of this problem in their weak ability to generalise outside of the training domain. We survey diverse research directions providing estimations of model generalisation ability and find that incorporating some of these measures in the training objectives leads to enhanced distributional robustness of neural models. Based on these findings, we present future research directions enhancing the robustness of LLMs.
SRW_24	Student Research Workshop	Language Generation	Retrieval-augmented Generation across Heterogeneous Knowledge	Wenhao Yu	Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.
SRW_25	Student Research Workshop	Information Retrieval and Text Mining	Neural Retriever and Go Beyond: A Thesis Proposal	Man Luo	Information Retriever (IR) aims to find the relevant documents (e.g. snippets, passages, and articles) to a given query at large scale.  IR plays an important role in many tasks such as open domain question answering and dialogue systems, where external knowledge is needed.  In the past, searching algorithms based on term matching have been widely used. Recently, neural-based algorithms (termed as neural retrievers) have gained more attention which can mitigate the limitations of traditional methods. Regardless of the success achieved by neural retrievers, they still face many challenges, e.g. suffering  from a small amount of training data and failing to answer simple entity-centric questions. Furthermore, most of the existing neural retrievers are developed for pure-text query. This prevents them from handling multi-modality queries (i.e. the query is composed of textual description and images). This proposal has two goals. First, we introduce methods to address the abovementioned issues of neural retrievers from three angles, new model architectures, IR-oriented pretraining tasks, and generating large scale training data. Second, we identify the future research direction and propose potential corresponding solution.
SRW_26	Student Research Workshop	Linguistic Theories, Cognitive Modeling and Pycholinguistics	Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation	Xiruo Ding, Kevin Lybarger, Justin Tauscher, Trevor Cohen	Cognitive distortions are counterproductive patterns of thinking that are one of the targets of cognitive behavioral therapy (CBT). These can be challenging for clinicians to detect, especially those without extensive CBT training or supervision. Text classification methods can approximate expert clinician judgment in the detection of frequently occurring cognitive distortions in text-based therapy messages. However, performance with infrequent distortions is relatively poor. In this study, we address this sparsity problem with two approaches: Data Augmentation and Domain-Specific Model. The first approach includes Easy Data Augmentation, back translation, and mixup techniques. The second approach utilizes a domain-specific pretrained language model, MentalBERT. To examine the viability of different data augmentation methods, we utilized a real-world dataset of texts between therapists and clients diagnosed with serious mental illness that was annotated for distorted thinking. We found that with optimized parameter settings, mixup was helpful for rare classes. Performance improvements with an augmented model, MentalBERT, exceed those obtained with data augmentation.
SRW_29	Student Research Workshop	NLP Applications	Understanding Long Document with Different Position-Aware Attentions	Hai Pham, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang	Despite several successes in document understanding, the practical task for long document understanding is largely under-explored due to several challenges in computation and how to efficiently absorb long multimodal input. Most current transformer-based approaches only deal with short documents and employ solely textual information for attention due to its prohibitive computation and memory limit. To address those issues in long document understanding, we explore different approaches in handling 1D and new 2D position-aware attention with essentially shortened context. Experimental results show that our proposed models have the advantages for this task based on various evaluation metrics. Furthermore, our model makes changes only to the attention and thus can be easily used for any transformer-based architecture. 
SRW_30	Student Research Workshop	Ethics, Bias and Fairness	Towards Gender Biased Language Classification: A Case Study with British English Archival Metadata Descriptions	Lucy Havens	This thesis-in-progress summarizes the work completed and potential directions for a Ph.D. project researching the classification of gender biased language.  Recognizing bias as inherent in language and thus inevitable in natural language processing systems, the project aims to make bias transparent.  An interdisciplinary methodology is applied to define gender bias, annotate documents according to that definition, and train classification models on the annotated dataset to identify types of gender bias.  Having created a gender biased language taxonomy and an annotated dataset, the project now moves towards the development of document classification models.  There are several directions the classifier development could follow.  The project would benefit from participation in the Student Research Workshop to discuss which direction would add the most valuable contribution to computational linguistics.
SRW_31	Student Research Workshop	Discourse and Pragmatics	What "Drives" the Use of Metaphorical Language? Negative Insights from Abstractness, Affect, Discourse Coherence and Contextualized Word Representations	Prisca Piccirilli, Sabine Schulte im Walde	Which features in a specific discourse trigger the use of metaphorical language, rather than literal alternatives? Many NLP approaches to metaphor rely on cognitive and (psycho)linguistic insights and have successfully applied models of discourse coherence, abstractness and affect, but this study indicates that these properties do not systematically explain metaphorical vs. literal preferences.
SRW_32	Student Research Workshop	Dialogue and Interactive Systems	Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation	Ryoma Sakaeda, Daisuke Kawahara	We aim to overcome the lack of diversity in responses of current dialogue systems and to develop a dialogue system that is engaging as a conversational partner. We propose a generator-evaluator model that evaluates multiple responses generated by a response generator and selects the best response by an evaluator. By generating multiple responses, we obtain diverse responses. We conduct human evaluations to compare the output of the proposed system with that of a baseline system. The results of the human evaluations showed that the proposed system's responses were often judged to be better than the baseline system's, and indicated the effectiveness of the proposed method.
SRW_33	Student Research Workshop	Efficient Methods in NLP	Impact of Training Instance Selection on Domain-Specific Entity Extraction using BERT	Eileen Salhofer, Xing Lan Liu, Roman Kern	State of the art performances for entity extraction tasks are achieved by supervised learning, specifically, by fine-tuning pretrained language models such as BERT. As a result, annotating application specific data is the first step in many use cases. However, no practical guidelines are available for annotation requirements. This work supports practitioners by empirically answering the frequently asked questions (1) how many training samples to annotate? (2) which examples to annotate? We found that BERT achieves up to 80% F1 when fine-tuned on only 70 training examples, especially on biomedical domain. The key features for guiding the selection of high performing training instances are identified to be pseudo-perplexity and sentence-length. The best training dataset constructed using our proposed selection strategy shows F1 score that is equivalent to a random selection with twice the sample size. The requirement of only a small number of training data implies cheaper implementations and opens door to wider range of applications.
SRW_37	Student Research Workshop	Machine Translation and Multilinguality	Analysing the Correlation between Lexical Ambiguity and Translation Quality in a Multimodal Setting using WordNet	Ali Hatami, Paul Buitelaar, Mihael Arcan	Multimodal Neural Machine Translation is focusing on using visual information to translate sentences in the source language into the target language. The main idea is to utilise information from visual modalities to promote the output quality of the text-based translation model. Although the recent multimodal strategies extract the most relevant visual information in images, the effectiveness of using visual information on translation quality changes based on the text dataset. Due to this, this work studies the impact of leveraging visual information in multimodal translation models of ambiguous sentences. Our experiments analyse the Multi30k evaluation dataset and calculate ambiguity scores of sentences based on the WordNet hierarchical structure. To calculate the ambiguity of a sentence, we extract the ambiguity scores for all nouns based on the number of senses in WordNet. The main goal is to find in which sentences, visual content can improve the text-based translation model. We report the correlation between the ambiguity scores and translation quality extracted for all sentences in the English-German dataset.
SRW_39	Student Research Workshop	Dialogue and Interactive Systems	Building a Personalized Dialogue System with Prompt-Tuning	Tomohito Kasahara, Daisuke Kawahara, Nguyen Tung, Shengzhe Li, Kenta Shinzato, Toshinori Sato	Dialogue systems without consistent responses are not attractive. In this study, we build a dialogue system that can respond based on a given character setting (persona) to bring consistency. Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models. The results of the automatic and manual evaluations in English and Japanese show that it is possible to build a dialogue system with more natural and personalized responses with less computational resources than fine-tuning.
SRW_40	Student Research Workshop	Machine Learning for NLP	MM-GATBT: Enriching Multimodal Representation Using Graph Attention Network	Seung Byum Seo, Hyoungwook Nam, Payam Delgosha	MM-GATBT: Enriching Multimodal Representation Using Graph Attention Network
SRW_42	Student Research Workshop	Syntax: Tagging, Chunking, and Parsing	Simulating Feature Structures with Simple Types	Valentin D. Richard	Feature structures have been several times considered to enrich categorial grammars in order to build fine-grained grammars. Most attempts to unify both frameworks either model categorial types as feature structures or add feature structures on top of categorial types. We pursue a different approach: using feature structure as categorial atomic types. In this article, we present a procedure to create, from a simplified HPSG grammar, an equivalent abstract categorial grammar (ACG). We represent a feature structure by the enumeration of its totally well-typed upper bounds, so that unification can be simulated as intersection. We implement this idea as a meta-ACG preprocessor.
SRW_47	Student Research Workshop	Speech and Multimodality	Investigating the effectiveness of various speaker embeddings for multi-speaker end-to-end speech synthesis system using small-sized speech data	Sheng-Yao Wang, Yi-Chin Huang	In this paper, we investigated the effectiveness of incorporating various speaker embeddings into an end-to-end speech synthesis system, for generating a unseen speaker's voice with small-sized speech data. To do so, we adopted learned speaker embeddings from various tasks, such as voice conversion and speaker verification. By combining the speaker embeddings using additive attention mechanism to an autoregressive-based speech synthesis framework, we could evaluate the performance of these embedding methods. To further enhance the speaker similarity and speech quality, the post-net for the output spectrogram sequence is replaced by a post-filter network. Experimental results showed that the proposed speech synthesis system with speaker embedding is capable of generating fluent arbitrary speech utterances of a unseen speaker with only few speech utterances. Besides, the post-filter network is helpful for enhancing the speaker similarity and speech naturalness of the output speech.
SRW_48	Student Research Workshop	Information Extraction	Dr. Livingstone, I presume? Polishing of foreign character identification in literary texts	Aleksandra Konovalova, Antonio Toral, Kristiina Taivalkoski-Shilov	Character identification is a key element for many narrative-related tasks. To implement it, the baseform of the name of the character (or lemma) needs to be identified, so different appearances of the same character in the narrative could be aligned. In this paper we tackle this problem in translated texts  (English–Finnish translation direction), where the challenge regarding lemmatizing foreign names in an agglutinative language appears. To solve this problem, we present and compare several methods. The results show that the method based on a search for the shortest version of the name proves to be the easiest, best performing (83.4% F1), and most resource-independent.
SRW_49	Student Research Workshop	Language Resources	Zuo Zhuan Ancient Chinese Dataset for Word Sense Disambiguation	Xiaomeng Pan, Hongfei Wang, Teruaki Oka, Mamoru Komachi	Word Sense Disambiguation (WSD) is a core task in Natural Language Processing (NLP). Ancient Chinese has rarely been used in WSD tasks, however, as no public dataset for ancient Chinese WSD tasks exists. Creation of an ancient Chinese dataset is considered a significant challenge because determining the most appropriate sense in a context is difficult and time-consuming owing to the different usages in ancient and modern Chinese. Actually, no public dataset for ancient Chinese WSD tasks exists. To solve the problem of ancient Chinese WSD, we annotate part of Pre-Qin (221 BC) text \textit{Zuo Zhuan} using a copyright-free dictionary to create a public sense-tagged dataset. Then, we apply a simple Nearest Neighbors (\mbox{k-NN}) method using a pre-trained language model to the dataset. Our code and dataset will be available on GitHub\footnote{\url{https://github.com/pxm427/Ancient-Chinese-WSD}}.
SRW_50	Student Research Workshop	Summarization	ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation	Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh	We present ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained on a large corpus of high-quality and diverse Vietnamese texts. We benchmark ViT5 on two downstream text generation tasks, Abstractive Text Summarization and Named Entity Recognition. Although Abstractive Text Summarization has been widely studied for the English language thanks to its rich and large source of data, there has been minimal research into the same task in Vietnamese, a much lower resource language. In this work, we perform exhaustive experiments on both Vietnamese Abstractive Summarization and Named Entity Recognition, validating the performance of ViT5 against many other pretrained Transformer-based encoder-decoder models. Our experiments show that ViT5 significantly outperforms existing models and achieves state-of-the-art results on Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5 is competitive against previous best results from pretrained encoder-based Transformer models. Further analysis shows the importance of context length during the self-supervised pretraining on downstream performance across different settings.
SRW_52	Student Research Workshop	Language Grounding to Vision, Robotics and Beyond	Compositional Generalization in Grounded Language Learning via Induced Model Sparsity	Sam Spilsbury, Alexander Ilin	We provide a study of how induced model sparsity can help achieve compositional generalization and better sample efficiency in grounded language learning problems. We consider simple language-conditioned navigation problems in a grid world environment with disentangled observations. We show that standard neural architectures do not always yield compositional generalization. To address this, we design an agent that contains a goal identification module that encourages sparse correlations between words in the instruction and attributes of objects, composing them together to find the goal. The output of the goal identification module is the input to a value iteration network planner. Our agent maintains a high level of performance on goals containing novel combinations of properties even when learning from a handful of demonstrations. We examine the internal representations of our agent and find the correct correspondences between words in its dictionary and attributes in the environment.
SRW_53	Student Research Workshop	Dialogue and Interactive Systems	How do people talk about images? A study on open-domain conversations with images.	Yi-Pei Chen, Nobuyuki Shimizu, Takashi Miyazaki, Hideki Nakayama	This paper explores how humans conduct conversations with images by investigating an open-domain image conversation dataset, ImageChat. We examined the conversations with images from the perspectives of  and . We found that utterances/conversations are not always related to the given image, and conversation topics diverge within three turns about half of the time. Besides image objects, more comprehensive non-object image information is also indispensable.  After inspecting the causes, we suggested that understanding the overall scenario of image and connecting objects based on their high-level attributes might be very helpful to generate more engaging open-domain conversations when an image is presented. We proposed enriching the image information with image caption and object tags based on our analysis. With our proposed  features, we improved automatic metrics including BLEU and Bert Score, and increased the diversity and image-relevancy of generated responses to the strong baseline. The result verifies that our analysis provides valuable insights and could facilitate future research on open-domain conversations with images.
SRW_54	Student Research Workshop	Ethics, Bias and Fairness	Text Style Transfer for Bias Mitigation using Masked Language Modeling	Ewoenam Kwaku Tokpo, Toon Calders	It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy.
SRW_55	Student Research Workshop	Speech and Multimodality	Preschool Children Speech Recognition for Early Childhood Intervention: Motivation and Challenges	Satwik Dutta, Dwight W. Irvin, John H. L. Hansen	Monitoring child development in terms of speech/language skills has a long-term impact on their overall growth. As student diversity continue to expand in US classrooms, there is a growing need to benchmark social engagement, both from a teacher-student perspective, as well as student-student content. Given various challenges with direct observation, deploying speech technology can assist in extracting meaningful information for teachers. These will help teachers to identify and respond to students in need, immediately impacting their early learning and interest. This study takes a deep dive into exploring hybrid ASR solutions for low-resource spontaneous preschool (3-5yrs) children (with and without developmental delays) speech, being involved in various activities, and interacting with teachers and peers in naturalistic classrooms. For the purpose of data augmentation, various out-of-domain corpora over a wide and limited age range, both scripted and spontaneous were considered. Acoustic models based on factorized time-delay neural networks, and both N-gram and neural language models were considered. Results indicate that young children have significantly different/developing articulation skills as compared to older children. Out-of-domain transcripts of interactions between young children and adults however enhances language model performance. Overall transcription of such data, including various non-linguistic markers, poses additional challenges.
SRW_59	Student Research Workshop	Question Answering	Eliciting Complex Relational Knowledge From Masked Language Models	Arun Sundaresan, Ming Hsu, Zhihao Zhang	We present results from a series of experiments that probe the ability of masked language models (MLMs), such as BERT and RoBERTa, to respond to general knowledge questions that do not have a single correct answer. Our investigation leverages the semantic fluency task from cognitive science, in which a variable number of exemplars from a semantic category (e.g., fruits) need to be produced in a specific order. It allows us to evaluate what MLMs know about common categories and their members, a representative type of one-to-many relational knowledge, and how they organize and query such knowledge. We developed incremental cloze tasks that reflect serial knowledge search, and show that MLMs, especially RoBERTa, are able to generate semantic fluency responses that strongly resemble responses from human subjects in both their content and dynamics. These findings contribute to the literature on whether and how masked language models can be used as knowledge bases, and also provide novel insights on their knowledge structure. 
SRW_60	Student Research Workshop	Ethics, Bias and Fairness	Differentially Private Instance Encoding against Privacy Attacks	Shangyu Xie, Yuan Hong	TextHide was recently proposed to protect the training data via instance encoding in natural language domain. Due to the lack of theoretic privacy guarantee, such instance encoding scheme has been shown to be vulnerable against privacy attacks, e.g., reconstruction attack. To address such limitation, we revise the instance encoding scheme with differential privacy and thus provide a provable guarantee against privacy attacks. The experimental results also show that the proposed scheme can defend against privacy attacks while ensuring learning utility (as a trade-off). 
SRW_63	Student Research Workshop	Question Answering	 A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context	Man Luo, Shuguang Chen, Chitta Baral	In the open book question answering (OBQA) task, selecting the relevant passages and sentences from distracting information is crucial to reason the answer to a question. HotpotQA dataset is designed to teach and evaluate systems to do both passage ranking and sentence selection. Many existing frameworks use separate models to select relevant passages and sentences respectively. Such systems not only have high complexity in terms of the parameters of models but also fail to take the advantage of training these two tasks together since one task can be beneficial for the other one.  In this work, we present a simple yet effective framework to address these limitations by jointly ranking passages and selecting sentences.  Furthermore, we propose consistency and similarity constraints to promote the correlation and interaction between passage ranking and sentence selection.The experiments demonstrate that our framework can achieve competitive results with previous systems and outperform the baseline by 28\% in terms of exact matching of relevant sentences on the HotpotQA dataset.
SRW_71	Student Research Workshop	Information Extraction	Multimodal Modeling of Task-Mediated Confusion	Camille Mince, Skye Rhomberg, Cecilia Alm, Reynold Bailey, Alex Ororbia	In order to build more human-like cognitive agents, systems capable of detecting various human emotions must be designed to respond appropriately. Confusion, the combination of an emotional and cognitive state, is under-explored. In this paper, we build upon prior work to develop models that detect confusion from three modalities: video (facial features), audio (prosodic features), and text (transcribed speech features). Our research improves the data collection process by allowing for continuous (as opposed to discrete) annotation of confusion levels. We also craft models based on recurrent neural networks (RNNs) given their ability to predict sequential data. In our experiments, we find that text and video modalities are the most important in predicting confusion while the explored audio features are relatively unimportant predictors of confusion in our data.
SRW_72	Student Research Workshop	Question Answering	Machine Narrative Comprehension in Fictional Characters Personality Prediction Task	Yisi Sang, Xiangyang Mou, Mo Yu, Dakuo Wang, Jing Li, Jeffrey Stanton	An NLP model that understands stories should also be able to understand the characters, which is underexplored till now. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character's personality based on the narratives. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We then proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which significantly improved the performance. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters.
SRW_73	Student Research Workshop	Information Retrieval and Text Mining	Divide & Conquer for Entailment-aware Multi-hop Evidence Retrieval	Fan Luo, Mihai Surdeanu	Lexical and semantic matches are commonly used as relevance measurements for information retrieval. Together they estimate the semantic equivalence between the query and the candidates. However, semantic equivalence is not the only relevance signal need to be considered when retrieving evidences for multi-hop questions. In this work, we demonstrate that textual entailment relation is another important relevance dimension that should be considered. To retrieve evidences that are either semantic equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval. We propose two ensemble models, EAR and EARnest, which tackle each of the sub-tasks separately with off-the-shelf retrieval models, and jointly retrieve sentences with the consideration of the diverse relevance signals. Experimental results on HotpotQA verify that our models not only significantly outperform all the single retrieval models it based on, but also more effective than two intuitive ensemble baseline models. 
SRW_76	Student Research Workshop	Interpretability and Analysis of Models for NLP	Probe-Less Probing of BERT's Layer-Wise Linguistic Knowledge with Masked Word Prediction	Tatsuya Aoyama, Nathan Schneider	The current study quantitatively (and qualitatively for an illustrative purpose) analyzes BERT’s layer-wise masked word prediction on an English corpus, and finds that (1) the layerwise localization of linguistic knowledge primarily shown in probing studies is replicated in a behavior-based design and (2) that syntactic and semantic information is encoded at different layers for words of different syntactic categories. Hypothesizing that the above results are correlated with the number of likely potential candidates of the masked word prediction, we also investigate how the results differ for tokens within multiword expressions.
SRW_77	Student Research Workshop	Information Extraction	CSSS: A Novel Candidate Summary Selection Strategy for Summary-level Extractive Summarization	Shuai Gong, Zhenfang Zhu, Wenqing Wu, Zhen Zhao, Dianyuan Zhang	Summary-level extractive summarization selects a summary with the highest semantic similarity to the document through a matching model, resulting in insufficient use of information between different candidate summaries. This paper presents a novel candidate summary selection strategy (CSSS), regarding candidate summaries as mathematical sets, and selecting the candidate summary has the highest semantic similarity to corresponding mutually exclusive sets. The strategy reduces the dependence on the matching model by exploiting the set relationship, could be effectively applied to both unsupervised and supervised extractive summarization. In order to fit this strategy better, we construct a contrastive learning framework to learn effective vector representation for each candidate summary. Experimental results show that we achieve state-of-the-art performance in both the unsupervised and supervised extractive summarization on CNN/DailyMail dataset. Experiments on Xsum and Reddit datasets also show the effectiveness of CSSS.
SRW_78	Student Research Workshop	Speech and Multimodality	Multimodal large language models for inclusive collaboration learning tasks	Armanda Lewis	This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills.  This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.
SRW_83	Student Research Workshop	Machine Learning for NLP	Neural Networks in a Product of Hyperbolic Spaces	Jun Takeuchi, Noriki Nishida, Hideki Nakayama	Machine learning in hyperbolic spaces has attracted much attention in natural language processing and many other fields. In particular, Hyperbolic Neural Networks (HNNs) have improved a wide variety of tasks, from machine translation to knowledge graph embedding. Although some studies have reported the effectiveness of embedding into the product of multiple hyperbolic spaces, HNNs have mainly been constructed in a single hyperbolic space, and their extension to product spaces has not been sufficiently studied. Therefore, we propose a novel method to extend a given HNN in a single space to a product of hyperbolic spaces. We apply our method to Hyperbolic Graph Convolutional Networks (HGCNs), extending several HNNs. Our model improved the graph node classification accuracy especially on datasets with tree-like structures. The results suggest that neural networks in a product of hyperbolic spaces can be more effective than in a single space in representing structural data.
SRW_84	Student Research Workshop	Dialogue and Interactive Systems	Explicit Use of Topicality in Dialogue Response Generation	Takumi Yoshikoshi, Hayato Atarashi, Takashi Kodama, Sadao Kurohashi	The current chat dialogue systems implicitly consider the topic given the context, but not explicitly. As a result, these systems often generate inconsistent responses with the topic of the moment. In this study, we propose a dialogue system that responds appropriately following the topic by selecting the entity with the highest “topicality.” In topicality estimation, the model is trained through self-supervised learning that regards entities that appear in both context and response as the topic entities. In response generation, the model is trained to generate topic-relevant responses based on the estimated topicality. Experimental results show that our proposed system can follow the topic more than the existing dialogue system that considers only the context.
SRW_86	Student Research Workshop	Dialogue and Interactive Systems	Automating Human Evaluation of Dialogue Systems	Sujan Reddy A	Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three prediction heads, to predict whether the system-generated output is natural, fluent, and informative. I observe that the proposed model achieves an average accuracy of around 77% over these 3 labels. I also design a baseline approach that uses three different BERT models to make the predictions. Based on experimental analysis, I find that using a shared model to compute the three labels performs better than three separate models.
SRW_87	Student Research Workshop	Information Extraction	Strong Heuristics for Named Entity Linking	Marko Čuljak, Andreas Spitz, Robert West, Akhil Arora	Named entity linking (NEL) in news is a challenging endeavour due to the frequency of unseen and emerging entities, which necessitates the use of unsupervised or zero-shot methods. However, such methods tend to come with caveats, such as no integration of suitable knowledge bases (like Wikidata) for emerging entities, a lack of scalability, and poor interpretability. Here, we consider person disambiguation in Quotebank, a massive corpus of speaker-attributed quotations from the news, and investigate the suitability of intuitive, lightweight, and scalable heuristics for NEL in web-scale corpora. Our best performing heuristic disambiguates 94% and 63% of the mentions on Quotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the proposed heuristics compare favourably to the state-of-the-art unsupervised and zero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as strong baselines for unsupervised and zero-shot entity linking.
SRW_88	Student Research Workshop	Sentiment Analysis and Stylistic Analysis	Static and Dynamic Speaker Modeling based on Graph Neural Network for Emotion Recognition in Conversation	Prakhar Saxena, Yin Jou Huang, Sadao Kurohashi	Each person has a unique personality which affects how they feel and convey emotions. Hence, speaker modeling is important for the task of emotion recognition in conversation (ERC). In this paper, we propose a novel graph-based ERC model which considers both conversational context and speaker personality. We model the internal state of the speaker (personality) as Static and Dynamic speaker state, where the Dynamic speaker state is modeled with a graph neural network based encoder. Experiments on benchmark dataset shows the effectiveness of our model. Our model outperforms baseline and other graph-based methods. Analysis of results also show the importance of explicit speaker modeling.
SRW_90	Student Research Workshop	Summarization	Few-shot fine-tuning SOTA summarization models for medical dialogues	David Fraile Navarro, Mark Dras, Shlomo Berkovsky	Abstractive summarization of medical dialogues presents a challenge for standard training approaches, given the paucity of suitable datasets. We explore the performance of state-of-the-art models with zero-shot and few-shot learning strategies and measure the impact of pretraining with general domain and dialogue-specific text on the summarization performance.
SRW_91	Student Research Workshop	Machine Learning for NLP	Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations	Antoine Simoulin, Benoit Crabbé	We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with Bert base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performances. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures.
SRW_94	Student Research Workshop	Speech and Multimodality	Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation	Gerard Sant, Gerard I. Gállego, Belen Alastruey, Marta Ruiz Costa-jussà	Transformer-based models have been achieving state-of-the-art results in several fields of Natural Language Processing. However, its direct application to speech tasks is not trivial. The nature of this sequences carries problems such as long sequence lengths and redundancy between adjacent tokens. Therefore, we believe that regular self-attention mechanism might not be well suited for it.   Different approaches have been proposed to overcome these problems, such as the use of efficient attention mechanisms. However, the use of these methods usually comes with a cost, which is a performance reduction caused by information loss. In this study, we present the Multiformer, a Transformer-based model which allows the use of different attention mechanisms on each head. By doing this, the model is able to bias the self-attention towards the extraction of more diverse token interactions, and the information loss is reduced. Finally, we perform an analysis of the head contributions, and we observe that those architectures where all heads relevance is uniformly distributed obtain better results. Our results show that mixing attention patterns along the different heads and layers outperforms our baseline by up to 0.7 BLEU.
SRW_97	Student Research Workshop	Representation Learning	Defending Compositionality in Emergent Languages	Michal Auersperger, Pavel Pecina	Compositionality has traditionally been understood as a major factor in productivity of language and, more broadly, human cognition. Yet, recently some research started to question its status showing that artificial neural networks are good at generalization even without noticeable compositional behavior. We argue some of these conclusions are too strong and/or incomplete. In the context of a two-agent communication game, we show that compositionality indeed seems essential for successful generalization when the evaluation is done on a suitable dataset.
SRW_98	Student Research Workshop	Speech and Multimodality	Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition	Aditya Yadavalli, Ganesh Sai Mirishkar, Anil Vuppala	Previous research has found that Acoustic Models (AM) of an Automatic Speech Recognition (ASR) system are susceptible to dialect variations within a language, thereby adversely affecting the ASR. To counter this, researchers have proposed to build a dialect-specific AM while keeping the Language Model (LM) constant for all the dialects. This study explores the effect of dialect mismatched LM by considering three different Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions. Therefore, this degradation has an adverse effect on the ASR even when dialect-specific AM is used. We show a degradation of up to 13.13 perplexity points when LM is used under mismatched conditions. Furthermore, we show a degradation of over 9% and over 15% in Character Error Rate (CER) and Word Error Rate (WER), respectively, in the ASR systems when using mismatched LMs over matched LMs.